{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import shap\n",
    "\n",
    "class FinancialExclusionAnalysis:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the financial exclusion analysis pipeline\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the CSV file containing financial inclusion data\n",
    "        \"\"\"\n",
    "        self.raw_data = pd.read_csv(data_path)\n",
    "        self.processed_data = None\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Comprehensive data preprocessing pipeline\n",
    "        \"\"\"\n",
    "        # 1. Handle Missing Values\n",
    "        # Identify numeric and categorical columns\n",
    "        numeric_columns = self.raw_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_columns = self.raw_data.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Impute missing values\n",
    "        # Numeric columns: median imputation\n",
    "        numeric_imputer = SimpleImputer(strategy='median')\n",
    "        self.raw_data[numeric_columns] = numeric_imputer.fit_transform(self.raw_data[numeric_columns])\n",
    "        \n",
    "        # Categorical columns: mode imputation\n",
    "        categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        self.raw_data[categorical_columns] = categorical_imputer.fit_transform(self.raw_data[categorical_columns])\n",
    "        \n",
    "        # 2. Encode Categorical Variables\n",
    "        # One-hot encoding for low-cardinality categorical variables\n",
    "        categorical_encoded = pd.get_dummies(self.raw_data[categorical_columns])\n",
    "        \n",
    "        # Label encoding for high-cardinality categorical variables\n",
    "        label_encoder = LabelEncoder()\n",
    "        high_cardinality_columns = [col for col in categorical_columns if self.raw_data[col].nunique() > 10]\n",
    "        for col in high_cardinality_columns:\n",
    "            self.raw_data[col + '_encoded'] = label_encoder.fit_transform(self.raw_data[col])\n",
    "        \n",
    "        # 3. Normalization\n",
    "        scaler = MinMaxScaler()\n",
    "        numeric_scaled = pd.DataFrame(scaler.fit_transform(self.raw_data[numeric_columns]), \n",
    "                                      columns=numeric_columns)\n",
    "        \n",
    "        # Combine processed features\n",
    "        self.processed_data = pd.concat([\n",
    "            numeric_scaled, \n",
    "            categorical_encoded, \n",
    "            self.raw_data[[col + '_encoded' for col in high_cardinality_columns]]\n",
    "        ], axis=1)\n",
    "        \n",
    "        # 4. Dimensionality Reduction\n",
    "        pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "        self.processed_data = pd.DataFrame(\n",
    "            pca.fit_transform(self.processed_data), \n",
    "            columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
    "        )\n",
    "        \n",
    "        # 5. Financial Engagement Score\n",
    "        self.calculate_financial_engagement_score()\n",
    "        \n",
    "        # 6. Financial Literacy Score\n",
    "        self.calculate_financial_literacy_score()\n",
    "        \n",
    "    def calculate_financial_engagement_score(self):\n",
    "        \"\"\"\n",
    "        Calculate a composite financial engagement score\n",
    "        \"\"\"\n",
    "        # Identify transaction and savings-related columns\n",
    "        transaction_cols = [col for col in self.raw_data.columns if 'transaction' in col.lower()]\n",
    "        savings_cols = [col for col in self.raw_data.columns if 'savings' in col.lower()]\n",
    "        \n",
    "        # Normalize transaction and savings metrics\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_transaction = scaler.fit_transform(self.raw_data[transaction_cols])\n",
    "        normalized_savings = scaler.fit_transform(self.raw_data[savings_cols])\n",
    "        \n",
    "        # Weighted average (can adjust weights based on domain knowledge)\n",
    "        transaction_weight = 0.6\n",
    "        savings_weight = 0.4\n",
    "        \n",
    "        financial_engagement_score = (\n",
    "            transaction_weight * normalized_transaction.mean(axis=1) + \n",
    "            savings_weight * normalized_savings.mean(axis=1)\n",
    "        )\n",
    "        \n",
    "        self.processed_data['financial_engagement_score'] = financial_engagement_score\n",
    "        \n",
    "    def calculate_financial_literacy_score(self):\n",
    "        \"\"\"\n",
    "        Calculate a composite financial literacy score\n",
    "        \"\"\"\n",
    "        # Use education and income as primary indicators\n",
    "        education_mapping = {\n",
    "            'Primary': 1,\n",
    "            'Secondary': 2,\n",
    "            'Tertiary': 3,\n",
    "            'University': 4\n",
    "        }\n",
    "        \n",
    "        # Normalize education and income\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_education = scaler.fit_transform(\n",
    "            self.raw_data['education'].map(education_mapping).values.reshape(-1, 1)\n",
    "        )\n",
    "        normalized_income = scaler.fit_transform(\n",
    "            self.raw_data['quintile'].values.reshape(-1, 1)\n",
    "        )\n",
    "        \n",
    "        # Weighted average (can adjust weights)\n",
    "        education_weight = 0.6\n",
    "        income_weight = 0.4\n",
    "        \n",
    "        financial_literacy_score = (\n",
    "            education_weight * normalized_education + \n",
    "            income_weight * normalized_income\n",
    "        )\n",
    "        \n",
    "        self.processed_data['financial_literacy_score'] = financial_literacy_score\n",
    "        \n",
    "    def cluster_financial_exclusion(self):\n",
    "        \"\"\"\n",
    "        Cluster individuals into inclusion/exclusion groups\n",
    "        \"\"\"\n",
    "        # Use K-Means clustering\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "        self.processed_data['exclusion_cluster'] = kmeans.fit_predict(self.processed_data)\n",
    "        \n",
    "        # Create binary exclusion label\n",
    "        self.processed_data['financial_exclusion_label'] = (\n",
    "            self.processed_data['exclusion_cluster'] == \n",
    "            kmeans.predict(self.processed_data.loc[\n",
    "                self.processed_data['financial_engagement_score'].idxmin()\n",
    "            ].values.reshape(1, -1)\n",
    "        )[0])\n",
    "        \n",
    "    def train_classification_models(self):\n",
    "        \"\"\"\n",
    "        Train multiple classification models to predict financial exclusion\n",
    "        \"\"\"\n",
    "        X = self.processed_data.drop('financial_exclusion_label', axis=1)\n",
    "        y = self.processed_data['financial_exclusion_label']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Standardize features for SVM and Logistic Regression\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Models to train\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
    "            'SVM': SVC(probability=True)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            # Train model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "            \n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            }\n",
    "            \n",
    "            # SHAP Explanability\n",
    "            if name in ['Random Forest', 'Logistic Regression']:\n",
    "                explainer = shap.TreeExplainer(model) if name == 'Random Forest' else shap.LinearExplainer(model, X_train_scaled)\n",
    "                shap_values = explainer.shap_values(X_test_scaled)\n",
    "                \n",
    "                results[name]['shap_summary'] = {\n",
    "                    'mean_abs_shap_value': np.abs(shap_values).mean(),\n",
    "                    'top_features': list(X.columns[np.abs(shap_values).mean(axis=0).argsort()[-5:][::-1]])\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Assuming the CSV is named 'financial_inclusion_data.csv'\n",
    "    analysis = FinancialExclusionAnalysis('updated_dataset.csv')\n",
    "    \n",
    "    # Run full analysis pipeline\n",
    "    analysis.preprocess_data()\n",
    "    analysis.cluster_financial_exclusion()\n",
    "    results = analysis.train_classification_models()\n",
    "    \n",
    "    # Print results\n",
    "    for model, metrics in results.items():\n",
    "        print(f\"\\n{model} Results:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with descriptive column names:\n",
      "  Unique 32-character long identifier of the interview   County  \\\n",
      "0                   0297462173dc461aa800fcec4e0bbe30    Mombasa   \n",
      "1                   02e0d75a58ca4562ad79b82ce7219716    Mombasa   \n",
      "2                   03451675e1444f859abf71c1aaec078d    Mombasa   \n",
      "3                   04acd31535be49cb89db86e698ef1f1c    Mombasa   \n",
      "4                   05dd5394d4f340ffb8bcb76e98ecbd36    Mombasa   \n",
      "\n",
      "   Individual Weights Selected Respondent Gender Cluster Type (rural/urban)  \\\n",
      "0                1836                       Male                      Urban   \n",
      "1                1506                       Male                      Urban   \n",
      "2                1336                       Male                      Urban   \n",
      "3                2493                       Male                      Urban   \n",
      "4                1980                       Male                      Urban   \n",
      "\n",
      "  Marital status of Respondent Education level of Respondent  \\\n",
      "0                      Widowed                     Secondary   \n",
      "1  Married/Living with partner                     Secondary   \n",
      "2  Married/Living with partner                     Secondary   \n",
      "3  Married/Living with partner                       Primary   \n",
      "4         Single/Never Married                     Secondary   \n",
      "\n",
      "  Estimated Wealth Quintile  A19. AGE OF RESPONDENT A20. Relation to HH Head  \\\n",
      "0                   Highest                      50        Head of household   \n",
      "1                   Highest                      59        Head of household   \n",
      "2                   Highest                      52        Head of household   \n",
      "3                   Highest                      75        Head of household   \n",
      "4                    Middle                      24           Other relative   \n",
      "\n",
      "   ... C2-2. Savings through mobile banking  \\\n",
      "0  ...                                        \n",
      "1  ...                                        \n",
      "2  ...                                        \n",
      "3  ...                                        \n",
      "4  ...                                        \n",
      "\n",
      "  C2_3. Savings through mobile money provider  \\\n",
      "0                                               \n",
      "1                                               \n",
      "2                                               \n",
      "3                                               \n",
      "4                                               \n",
      "\n",
      "  C2_9. Registered on Mobile money C2_10. Registered on Mobile banking  \\\n",
      "0                                                                        \n",
      "1                                                                        \n",
      "2                                                                        \n",
      "3                                                                        \n",
      "4                                                                        \n",
      "\n",
      "  C2_12. Loan from mobile banking  \\\n",
      "0                                   \n",
      "1                                   \n",
      "2                                   \n",
      "3                                   \n",
      "4                                   \n",
      "\n",
      "  C2_13. Mobile money loans (e.g Fuliza). Did you use this in the last 12 months?  \\\n",
      "0                                                                                   \n",
      "1                                                                                   \n",
      "2                                                                                   \n",
      "3                                                                                   \n",
      "4                                                                                   \n",
      "\n",
      "  C3_1. Capital Markets Intermediary C3_2. Pensions  \\\n",
      "0                                 No             No   \n",
      "1                  Refused to Answer             No   \n",
      "2                                 No             No   \n",
      "3                                 No             No   \n",
      "4                                 No             No   \n",
      "\n",
      "  QC4. Trusted financial provider C5. Highest Interest Rate Perception  \n",
      "0                          A bank                               A bank  \n",
      "1                          A bank                               A bank  \n",
      "2                          A bank                              A Sacco  \n",
      "3                          A bank                               A bank  \n",
      "4                          A bank        \"Don't know(DO NOT READ OUT)\"  \n",
      "\n",
      "[5 rows x 77 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Create a mapping of short column names to descriptive labels\n",
    "columns_info_path = 'labels.xlsx'  # Replace with the actual file containing column names and labels\n",
    "column_info = pd.read_excel(columns_info_path)  # This file contains Variable, Position, and Label\n",
    "\n",
    "# Map short column names to detailed descriptions\n",
    "column_mapping = dict(zip(column_info['Variable'], column_info['Label']))\n",
    "\n",
    "# 2. Load the actual dataset\n",
    "data_path = 'data2021.csv'  # Replace with the actual dataset file\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 3. Rename the columns in the actual dataset\n",
    "data.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Check for duplicates and make labels unique if necessary\n",
    "if data.columns.duplicated().any():\n",
    "    data.columns = pd.io.parsers.ParserBase({'names': data.columns})._maybe_dedup_names(data.columns)\n",
    "\n",
    "# Display the updated DataFrame with descriptive column names\n",
    "print(\"Updated DataFrame with descriptive column names:\")\n",
    "print(data.head())\n",
    "\n",
    "# Save the updated dataset if required\n",
    "data.to_csv('updated_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import shap\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 0.75 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    return pd.read_csv(data_path)\n",
    "\n",
    "data_path = 'updated_dataset.csv'\n",
    "\n",
    "start_time = time.time()\n",
    "raw_data = load_data(data_path)\n",
    "print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Handle missing values\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[numeric_columns] = numeric_imputer.fit_transform(data[numeric_columns])\n",
    "    data[categorical_columns] = categorical_imputer.fit_transform(data[categorical_columns])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_encoded = pd.get_dummies(data[categorical_columns])\n",
    "    label_encoder = LabelEncoder()\n",
    "    high_cardinality_columns = [col for col in categorical_columns if data[col].nunique() > 10]\n",
    "    for col in high_cardinality_columns:\n",
    "        data[col + '_encoded'] = label_encoder.fit_transform(data[col])\n",
    "    \n",
    "    # Normalize numeric data\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_scaled = pd.DataFrame(scaler.fit_transform(data[numeric_columns]), columns=numeric_columns)\n",
    "    \n",
    "    # Combine all processed features\n",
    "    processed_data = pd.concat([\n",
    "        numeric_scaled,\n",
    "        categorical_encoded,\n",
    "        data[[col + '_encoded' for col in high_cardinality_columns]]\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Dimensionality reduction using PCA\n",
    "    pca = PCA(n_components=0.95)\n",
    "    processed_data = pd.DataFrame(\n",
    "        pca.fit_transform(processed_data), \n",
    "        columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
    "    )\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "start_time = time.time()\n",
    "processed_data = preprocess_data(raw_data)\n",
    "print(f\"Data preprocessed in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_financial_engagement_score(data, processed_data):\n",
    "    transaction_cols = [col for col in data.columns if 'transaction' in col.lower()]\n",
    "    savings_cols = [col for col in data.columns if 'savings' in col.lower()]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_transaction = scaler.fit_transform(data[transaction_cols])\n",
    "    normalized_savings = scaler.fit_transform(data[savings_cols])\n",
    "    \n",
    "    financial_engagement_score = (\n",
    "        0.6 * normalized_transaction.mean(axis=1) + \n",
    "        0.4 * normalized_savings.mean(axis=1)\n",
    "    )\n",
    "    processed_data['financial_engagement_score'] = financial_engagement_score\n",
    "    return processed_data\n",
    "\n",
    "start_time = time.time()\n",
    "processed_data = calculate_financial_engagement_score(raw_data, processed_data)\n",
    "print(f\"Financial engagement score calculated in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_financial_literacy_score(data, processed_data):\n",
    "    education_mapping = {'Primary': 1, 'Secondary': 2, 'Tertiary': 3, 'University': 4}\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_education = scaler.fit_transform(\n",
    "        data['education'].map(education_mapping).values.reshape(-1, 1)\n",
    "    )\n",
    "    normalized_income = scaler.fit_transform(data['quintile'].values.reshape(-1, 1))\n",
    "    \n",
    "    financial_literacy_score = (\n",
    "        0.6 * normalized_education + \n",
    "        0.4 * normalized_income\n",
    "    )\n",
    "    processed_data['financial_literacy_score'] = financial_literacy_score\n",
    "    return processed_data\n",
    "\n",
    "start_time = time.time()\n",
    "processed_data = calculate_financial_literacy_score(raw_data, processed_data)\n",
    "print(f\"Financial literacy score calculated in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_financial_exclusion(processed_data):\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    processed_data['exclusion_cluster'] = kmeans.fit_predict(processed_data)\n",
    "    processed_data['financial_exclusion_label'] = (\n",
    "        processed_data['exclusion_cluster'] == \n",
    "        kmeans.predict(processed_data.loc[\n",
    "            processed_data['financial_engagement_score'].idxmin()\n",
    "        ].values.reshape(1, -1))[0]\n",
    "    )\n",
    "    return processed_data\n",
    "\n",
    "start_time = time.time()\n",
    "processed_data = cluster_financial_exclusion(processed_data)\n",
    "print(f\"Financial exclusion clustered in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_models(processed_data):\n",
    "    X = processed_data.drop('financial_exclusion_label', axis=1)\n",
    "    y = processed_data['financial_exclusion_label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
    "        'SVM': SVC(probability=True)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model_start_time = time.time()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "        results[name] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1}\n",
    "        \n",
    "        if name in ['Random Forest', 'Logistic Regression']:\n",
    "            explainer = shap.TreeExplainer(model) if name == 'Random Forest' else shap.LinearExplainer(model, X_train_scaled)\n",
    "            shap_values = explainer.shap_values(X_test_scaled)\n",
    "            results[name]['shap_summary'] = {\n",
    "                'mean_abs_shap_value': np.abs(shap_values).mean(),\n",
    "                'top_features': list(X.columns[np.abs(shap_values).mean(axis=0).argsort()[-5:][::-1]])\n",
    "            }\n",
    "        \n",
    "        print(f\"{name} trained in {time.time() - model_start_time:.2f} seconds\")\n",
    "    return results\n",
    "\n",
    "start_time = time.time()\n",
    "results = train_classification_models(processed_data)\n",
    "print(f\"All models trained in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model} Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "div",
   "language": "python",
   "name": "div"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
